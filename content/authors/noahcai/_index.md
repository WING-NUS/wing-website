---
# Display name
title: Noah Xinjie Cai

# Full Name (for SEO)
first_name: Xinjie Noah
last_name: Cai

# Date for sorting
date: '2026-01-16'

# Is this the primary user of the site?
superuser: false

# Role/position
role: Master's Student (Jan '26)

# Organizations/Affiliations
organizations:
  - name: National University of Singapore, School of Computing
    url: 'http://www.comp.nus.edu.sg'

# Short bio (displayed in user profile at end of posts)
bio: Master's Student Jan 2026 Intake

interests:
  - Trustworthy AI
  - Model Sycophancy

education:
  courses:
    - course: MComp in Computer Science
      institution: National University of Singapore
      year: 2025-Present
    - course: BComp in Computer Science
      institution: National University of Singapore
      year: 2021-2025

# Social/Academic Networking
# For available icons, see: https://docs.hugoblox.com/getting-started/page-builder/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "#contact" for contact widget.
social:
  - icon: envelope
    icon_pack: fas
    link: 'e0725908@u.nus.edu'
  - icon: github
    icon_pack: fab
    link: https://github.com/noahxinjie
# Link to a PDF of your resume/CV from the About widget.
# To enable, copy your resume/CV to `static/files/cv.pdf` and uncomment the lines below.
# - icon: cv
#   icon_pack: ai
#   link: files/cv.pdf

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: 'e0725908@u.nus.edu'

# Highlight the author in author lists? (true/false)
highlight_name: false

# Organizational groups that you belong to (for People widget)
#   Set this to `[]` or comment out if you are not using People widget.
user_groups:
  - Graduate Students
#  - Researchers
---

Noah Xinjie Cai is an M.Comp. student in the School of Computing at the National University of Singapore, currently supervised by Prof. Min-Yen Kan. His research interests lie in the broad field of Trustworthy AI. Specifically, he is currently investigating LLM sycophancy and developing possible mitigation strategies through model alignment.
